{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a822e18",
   "metadata": {},
   "source": [
    "# NumPy Interview Exercise: Low-Rank Adapted Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf153a",
   "metadata": {},
   "source": [
    "## Part 1: Single Low-Rank Adapter\n",
    "\n",
    "### Inputs\n",
    "\n",
    "You are given the following NumPy arrays:\n",
    "\n",
    "- `x`: input activations, shape `(batch, d_in)`\n",
    "- `W`: base weight matrix, shape `(d_out, d_in)`\n",
    "- `A`: low-rank left factor, shape `(r, d_in)`\n",
    "- `B`: low-rank right factor, shape `(d_out, r)`\n",
    "- `alpha`: scalar scaling factor (float, default = 1.0)\n",
    "\n",
    "### Output\n",
    "\n",
    "Return an array `y` of shape `(batch, d_out)` defined as:\n",
    "\n",
    "$$\n",
    "y = x W^T + \\alpha \\cdot x A^T B^T\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Constraints\n",
    "\n",
    "1. **Do not explicitly construct**:\n",
    "   - `W_eff = W + B @ A`, or\n",
    "   - `deltaW = B @ A`\n",
    "2. You may use:\n",
    "   - `@`, `np.matmul`, `np.dot`, or `np.einsum`\n",
    "3. The function should raise a clear `ValueError` if shapes are incompatible.\n",
    "4. Assume inputs are `float32` or `float64`; dtype promotion may follow NumPy defaults.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080483d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adapted_linear(x: np.ndarray,\n",
    "                   W: np.ndarray,\n",
    "                   A: np.ndarray,\n",
    "                   B: np.ndarray,\n",
    "                   alpha: float = 1.0) -> np.ndarray:\n",
    "    ...\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "batch, d_in, d_out, r = 4, 6, 5, 2\n",
    "x = np.random.randn(batch, d_in).astype(np.float32)\n",
    "W = np.random.randn(d_out, d_in).astype(np.float32)\n",
    "A = np.random.randn(r, d_in).astype(np.float32)\n",
    "B = np.random.randn(d_out, r).astype(np.float32)\n",
    "alpha = 0.3\n",
    "\n",
    "y = adapted_linear(x, W, A, B, alpha)\n",
    "\n",
    "# reference (allowed in test)\n",
    "W_eff = W + alpha * (B @ A)\n",
    "y_ref = x @ W_eff.T\n",
    "\n",
    "print(np.max(np.abs(y - y_ref)))\n",
    "# ~e-7 for float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13596e",
   "metadata": {},
   "source": [
    "## Part 2: Multiple Low-Rank Adapters\n",
    "\n",
    "In this extension, the linear layer is adapted by **multiple low-rank terms**, each with its own coefficient.\n",
    "\n",
    "### Additional Inputs\n",
    "\n",
    "- `As`: array of low-rank left factors, shape `(n_adapters, r, d_in)`\n",
    "- `Bs`: array of low-rank right factors, shape `(n_adapters, d_out, r)`\n",
    "- `coeffs`: mixing coefficients, shape `(n_adapters,)`\n",
    "\n",
    "### Output Definition\n",
    "\n",
    "The output is defined as:\n",
    "\n",
    "$$\n",
    "y = x W^T + alpha * sum_{i=1..n_adapters} coeffs[i] * x A_i^T B_i^T\n",
    "$$\n",
    "\n",
    "### Constraints\n",
    "\n",
    "- **Do not construct any full `(d_out, d_in)` matrix**, including:\n",
    "  - `B_i @ A_i`\n",
    "  - `W_eff = W + sum_i coeffs[i] * (B_i @ A_i)`\n",
    "- Avoid creating large intermediate tensors such as `(n_adapters, d_out, d_in)`.\n",
    "- Broadcasting, reshaping, and transposing are allowed and expected.\n",
    "- Raise a clear `ValueError` if input shapes are incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapted_linear_multi(x: np.ndarray,\n",
    "                          W: np.ndarray,\n",
    "                          As: np.ndarray,\n",
    "                          Bs: np.ndarray,\n",
    "                          coeffs: np.ndarray,\n",
    "                          alpha: float = 1.0) -> np.ndarray:\n",
    "    ...\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "b, d_in, d_out, r, n = 4, 6, 5, 2, 3\n",
    "\n",
    "x = np.random.randn(b, d_in).astype(np.float32)\n",
    "W = np.random.randn(d_out, d_in).astype(np.float32)\n",
    "As = np.random.randn(n, r, d_in).astype(np.float32)\n",
    "Bs = np.random.randn(n, d_out, r).astype(np.float32)\n",
    "coeffs = np.random.randn(n).astype(np.float32)\n",
    "alpha = 0.3\n",
    "\n",
    "y = adapted_linear_multi(x, W, As, Bs, coeffs, alpha)\n",
    "\n",
    "W_eff = W.copy()\n",
    "for i in range(n):\n",
    "    W_eff += alpha * coeffs[i] * (Bs[i] @ As[i])\n",
    "\n",
    "y_ref = x @ W_eff.T\n",
    "print(np.max(np.abs(y - y_ref)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
